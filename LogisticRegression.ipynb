{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LogisticRegression.ipynb",
      "provenance": [],
      "mount_file_id": "1Bomlw67ER5BYucgM_AcyxUgHpFlWdcIx",
      "authorship_tag": "ABX9TyOjz/LjPypxfEioQqh3RDiX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antoh/DataScience/blob/main/LogisticRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbGtjEL48Wi1"
      },
      "source": [
        "Changing from regression to classification:\n",
        "\n",
        "- change last activation to Sigmoid to get a value between 0 and 1\n",
        "- use as many output nodes as you have choices (2 choices = 2 nodes)\n",
        "- use CrossEntropyLoss to better measure error for classification\n",
        "\n",
        "We are using \"logistic regression\" for classification tasks because the logistic function (aka, sigmoid) looks like a sideways \"S\" and ranges between 0 and 1. It essentially produces a \"percent likelihood\" that the data is in class A or B, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwBbj5ki8bB8"
      },
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import sklearn.metrics"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        },
        "id": "yNlAWFi28odh",
        "outputId": "ccaf5f33-9ee7-40b9-9c31-1038e75f0cd5"
      },
      "source": [
        "adultdf = pd.read_csv(\"/content/drive/MyDrive/DataScience/LogisticRegression/adult.data\", header=None)\n",
        "adultdf.columns = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"educationNum\",\n",
        "                  \"maritalStatus\", \"occupation\", \"relationship\", \"race\",\n",
        "                  \"sex\", \"capitalGain\", \"capitalLoss\", \"hoursPerWeek\",\n",
        "                  \"nativeCountry\", \"income\"]\n",
        "adultdf"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>workclass</th>\n",
              "      <th>fnlwgt</th>\n",
              "      <th>education</th>\n",
              "      <th>educationNum</th>\n",
              "      <th>maritalStatus</th>\n",
              "      <th>occupation</th>\n",
              "      <th>relationship</th>\n",
              "      <th>race</th>\n",
              "      <th>sex</th>\n",
              "      <th>capitalGain</th>\n",
              "      <th>capitalLoss</th>\n",
              "      <th>hoursPerWeek</th>\n",
              "      <th>nativeCountry</th>\n",
              "      <th>income</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>39</td>\n",
              "      <td>State-gov</td>\n",
              "      <td>77516</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>Adm-clerical</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>2174</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>50</td>\n",
              "      <td>Self-emp-not-inc</td>\n",
              "      <td>83311</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Exec-managerial</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>38</td>\n",
              "      <td>Private</td>\n",
              "      <td>215646</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9</td>\n",
              "      <td>Divorced</td>\n",
              "      <td>Handlers-cleaners</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>53</td>\n",
              "      <td>Private</td>\n",
              "      <td>234721</td>\n",
              "      <td>11th</td>\n",
              "      <td>7</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Handlers-cleaners</td>\n",
              "      <td>Husband</td>\n",
              "      <td>Black</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>28</td>\n",
              "      <td>Private</td>\n",
              "      <td>338409</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Prof-specialty</td>\n",
              "      <td>Wife</td>\n",
              "      <td>Black</td>\n",
              "      <td>Female</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>Cuba</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32556</th>\n",
              "      <td>27</td>\n",
              "      <td>Private</td>\n",
              "      <td>257302</td>\n",
              "      <td>Assoc-acdm</td>\n",
              "      <td>12</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Tech-support</td>\n",
              "      <td>Wife</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>38</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32557</th>\n",
              "      <td>40</td>\n",
              "      <td>Private</td>\n",
              "      <td>154374</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Machine-op-inspct</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&gt;50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32558</th>\n",
              "      <td>58</td>\n",
              "      <td>Private</td>\n",
              "      <td>151910</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9</td>\n",
              "      <td>Widowed</td>\n",
              "      <td>Adm-clerical</td>\n",
              "      <td>Unmarried</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32559</th>\n",
              "      <td>22</td>\n",
              "      <td>Private</td>\n",
              "      <td>201490</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>Adm-clerical</td>\n",
              "      <td>Own-child</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32560</th>\n",
              "      <td>52</td>\n",
              "      <td>Self-emp-inc</td>\n",
              "      <td>287927</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Exec-managerial</td>\n",
              "      <td>Wife</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>15024</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&gt;50K</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>32561 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       age          workclass  fnlwgt  ... hoursPerWeek   nativeCountry  income\n",
              "0       39          State-gov   77516  ...           40   United-States   <=50K\n",
              "1       50   Self-emp-not-inc   83311  ...           13   United-States   <=50K\n",
              "2       38            Private  215646  ...           40   United-States   <=50K\n",
              "3       53            Private  234721  ...           40   United-States   <=50K\n",
              "4       28            Private  338409  ...           40            Cuba   <=50K\n",
              "...    ...                ...     ...  ...          ...             ...     ...\n",
              "32556   27            Private  257302  ...           38   United-States   <=50K\n",
              "32557   40            Private  154374  ...           40   United-States    >50K\n",
              "32558   58            Private  151910  ...           40   United-States   <=50K\n",
              "32559   22            Private  201490  ...           20   United-States   <=50K\n",
              "32560   52       Self-emp-inc  287927  ...           40   United-States    >50K\n",
              "\n",
              "[32561 rows x 15 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKg5NV4T80fk"
      },
      "source": [
        "Need to turn some of these categorical columns into \"one-hot\" columns, such as workclass: Private = {0,0,0}, Self-emp-not-inc = {0,1,0}, etc. This way, the neural net does not see labels like Private = 1, Self-emp-not-inc = 2, etc. which may lead it to believe Private is \"close to\" Self-emp-not-inc, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "cFqhtNC987Hm",
        "outputId": "ec276a61-08d7-4a72-89a0-284ed6aa02fd"
      },
      "source": [
        "# example of pandas' get_dummies on a single column:\n",
        "pd.get_dummies(adultdf[[\"workclass\"]], prefix=[\"workclass\"], columns=[\"workclass\"])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>workclass_ ?</th>\n",
              "      <th>workclass_ Federal-gov</th>\n",
              "      <th>workclass_ Local-gov</th>\n",
              "      <th>workclass_ Never-worked</th>\n",
              "      <th>workclass_ Private</th>\n",
              "      <th>workclass_ Self-emp-inc</th>\n",
              "      <th>workclass_ Self-emp-not-inc</th>\n",
              "      <th>workclass_ State-gov</th>\n",
              "      <th>workclass_ Without-pay</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32556</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32557</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32558</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32559</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32560</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>32561 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       workclass_ ?  ...  workclass_ Without-pay\n",
              "0                 0  ...                       0\n",
              "1                 0  ...                       0\n",
              "2                 0  ...                       0\n",
              "3                 0  ...                       0\n",
              "4                 0  ...                       0\n",
              "...             ...  ...                     ...\n",
              "32556             0  ...                       0\n",
              "32557             0  ...                       0\n",
              "32558             0  ...                       0\n",
              "32559             0  ...                       0\n",
              "32560             0  ...                       0\n",
              "\n",
              "[32561 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "y1nJ1N2T9GnS",
        "outputId": "a26129c2-7457-4e33-a781-4b840f04ec56"
      },
      "source": [
        "onehot_columns = [\"relationship\"]\n",
        "x = pd.get_dummies(adultdf[onehot_columns],\n",
        "                    prefix=onehot_columns,\n",
        "                    columns=onehot_columns)\n",
        "x"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>relationship_ Husband</th>\n",
              "      <th>relationship_ Not-in-family</th>\n",
              "      <th>relationship_ Other-relative</th>\n",
              "      <th>relationship_ Own-child</th>\n",
              "      <th>relationship_ Unmarried</th>\n",
              "      <th>relationship_ Wife</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32556</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32557</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32558</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32559</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32560</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>32561 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       relationship_ Husband  ...  relationship_ Wife\n",
              "0                          0  ...                   0\n",
              "1                          1  ...                   0\n",
              "2                          0  ...                   0\n",
              "3                          1  ...                   0\n",
              "4                          0  ...                   1\n",
              "...                      ...  ...                 ...\n",
              "32556                      0  ...                   1\n",
              "32557                      1  ...                   0\n",
              "32558                      0  ...                   0\n",
              "32559                      0  ...                   0\n",
              "32560                      0  ...                   1\n",
              "\n",
              "[32561 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj925Hak--Ff"
      },
      "source": [
        "Because we are doing classification, we will use CrossEntropyLoss for our criterion function. The documentation shows that the \"input\" (the last layer of the network) should have 2 neurons because we have two classes (<=50K, >50K) for our target. The docs also show we need to make our y target values a single number that is 0 or 1 (since we have two classes). We do not use \"onehot\" encoding on these y values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXsRMFaR-9pT",
        "outputId": "8ac6a4b7-78bd-4a5f-941b-aa11f9f4f845"
      },
      "source": [
        "y = pd.Series([x == ' <=50K' for x in list(adultdf[\"income\"])])\n",
        "y"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         True\n",
              "1         True\n",
              "2         True\n",
              "3         True\n",
              "4         True\n",
              "         ...  \n",
              "32556     True\n",
              "32557    False\n",
              "32558     True\n",
              "32559     True\n",
              "32560    False\n",
              "Length: 32561, dtype: bool"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DrfxUH1_cKG"
      },
      "source": [
        "**NOTE!!!**\n",
        "\n",
        "There are only 24% of one class, so our model has to at least get less error than this! It can simply learn to always say \"1\" or whatever, ignoring the input data, and still be wrong only 24% of the time!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3W6mvORs_W0U",
        "outputId": "2e8f181a-d91e-46e9-b93a-a2aa08dba9a1"
      },
      "source": [
        "y_true = len(list(filter(lambda x: x, y)))\n",
        "y_false = len(list(filter(lambda x: not x, y)))\n",
        "print(y_true, y_false, 1-float(y_true)/float(y_true+y_false))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24720 7841 0.2408095574460244\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hE3xlr99_to4",
        "outputId": "a3a03f94-981d-4c9f-dc30-a0630bf1d7c3"
      },
      "source": [
        "indexes = pd.Series(y.sample(frac=1.0, random_state=0).index)\n",
        "train_idxs = indexes.iloc[range(0, int(len(indexes)*0.6))]\n",
        "val_idxs = indexes.iloc[range(int(len(indexes)*0.6), int(len(indexes)*0.8))]\n",
        "test_idxs = indexes.iloc[range(int(len(indexes)*0.8), len(indexes))]\n",
        "train_x = x.iloc[train_idxs]\n",
        "val_x = x.iloc[val_idxs]\n",
        "test_x = x.iloc[test_idxs]\n",
        "train_y = y.iloc[train_idxs]\n",
        "val_y = y.iloc[val_idxs]\n",
        "test_y = y.iloc[test_idxs]\n",
        "train_y"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22278     True\n",
              "8950      True\n",
              "7838      True\n",
              "16505     True\n",
              "19140    False\n",
              "         ...  \n",
              "14525     True\n",
              "26826    False\n",
              "18552     True\n",
              "17957    False\n",
              "27290    False\n",
              "Length: 19536, dtype: bool"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5djT3Rgj_xjJ",
        "outputId": "30e81bc1-40a7-461a-e774-e495fa4a898c"
      },
      "source": [
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(train_x.shape[1], 100), # compute number of columns from shape\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Dropout(p=0.5), # 50% of weights will not be trained each epoch\n",
        "    torch.nn.Linear(100, 2),\n",
        "    torch.nn.Sigmoid()\n",
        ")\n",
        "model.cuda()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=6, out_features=100, bias=True)\n",
              "  (1): ReLU()\n",
              "  (2): Dropout(p=0.5, inplace=False)\n",
              "  (3): Linear(in_features=100, out_features=2, bias=True)\n",
              "  (4): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Y7nDxpAA4wr"
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gRTQtbQBPXF",
        "outputId": "b271a689-1e83-4ca3-be2d-e3a8c97d726a"
      },
      "source": [
        "train_x_gpu = torch.tensor(train_x.to_numpy()).float().cuda()\n",
        "val_x_gpu = torch.tensor(val_x.to_numpy()).float().cuda()\n",
        "train_y_gpu = torch.tensor(train_y.to_numpy()).long().cuda()\n",
        "val_y_gpu = torch.tensor(val_y.to_numpy()).long().cuda()\n",
        "train_x_gpu"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0., 1., 0.],\n",
              "        [0., 1., 0., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., 1., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0.]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7W8v2fQBPIw",
        "outputId": "8e95ff7d-1e29-42ad-84a2-871cdf28cdab"
      },
      "source": [
        "for epoch in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    train_pred = model(train_x_gpu)\n",
        "    train_loss = criterion(train_pred, train_y_gpu)\n",
        "    val_pred = model(val_x_gpu)\n",
        "    val_loss = criterion(val_pred, val_y_gpu)\n",
        "    \n",
        "    # compute % accuracy (actually, error)\n",
        "    # first, convert the two outputs into a single prediction\n",
        "    train_pred_label = [0 if pred[0] > pred[1] else 1 for pred in train_pred.cpu().detach().tolist()]\n",
        "    val_pred_label = [0 if pred[0] > pred[1] else 1 for pred in val_pred.cpu().detach().tolist()]\n",
        "    train_error = 1.0 - sklearn.metrics.accuracy_score(train_y_gpu.cpu().tolist(), train_pred_label)\n",
        "    val_error = 1.0 - sklearn.metrics.accuracy_score(val_y_gpu.cpu().tolist(), val_pred_label)\n",
        "    print(train_loss.item(), val_loss.item(), train_error, val_error)\n",
        "    train_loss.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6983808279037476 0.6978382468223572 0.5700757575757576 0.5703316953316953\n",
            "0.6981967687606812 0.6976204514503479 0.5681818181818181 0.5612714987714988\n",
            "0.6972208023071289 0.697152853012085 0.5570741195741196 0.5578931203931203\n",
            "0.6969624757766724 0.696898877620697 0.5545659295659295 0.5543611793611793\n",
            "0.6965380907058716 0.6957377195358276 0.5527231777231778 0.5419226044226044\n",
            "0.6962052583694458 0.6954973936080933 0.546990171990172 0.5336302211302211\n",
            "0.6956281065940857 0.6955417990684509 0.5405917280917281 0.5290233415233415\n",
            "0.6951631307601929 0.6949793100357056 0.5340909090909092 0.5313267813267813\n",
            "0.6945670247077942 0.6943801641464233 0.5256449631449631 0.5187346437346437\n",
            "0.6943113207817078 0.6935708522796631 0.5261568386568387 0.5076781326781327\n",
            "0.6937029957771301 0.6941842436790466 0.5099303849303849 0.5216523341523341\n",
            "0.6936092972755432 0.6933209300041199 0.5148443898443898 0.5042997542997543\n",
            "0.6929850578308105 0.6929388046264648 0.5032248157248157 0.4993857493857494\n",
            "0.6927643418312073 0.6928085684776306 0.5023034398034398 0.5030712530712531\n",
            "0.6921638250350952 0.6919564604759216 0.4937551187551188 0.4933968058968059\n",
            "0.6918693780899048 0.6916086077690125 0.4903255528255528 0.48234029484029484\n",
            "0.6913472414016724 0.6911991238594055 0.48157248157248156 0.48234029484029484\n",
            "0.6911523342132568 0.6910903453826904 0.4810606060606061 0.47850122850122845\n",
            "0.690227746963501 0.6902696490287781 0.4697481572481572 0.4734336609336609\n",
            "0.6900686025619507 0.6896246075630188 0.4702088452088452 0.45838452088452086\n",
            "0.6895979642868042 0.6888144016265869 0.4578214578214578 0.4447174447174447\n",
            "0.6893683671951294 0.6888871788978577 0.4582309582309583 0.4502457002457002\n",
            "0.6887328028678894 0.6886731386184692 0.44579238329238324 0.4479422604422605\n",
            "0.6883580684661865 0.6875749826431274 0.4443079443079443 0.4272113022113022\n",
            "0.6881126165390015 0.6877373456954956 0.4380118755118755 0.4279791154791155\n",
            "0.6876469850540161 0.687166154384613 0.43212530712530717 0.4275184275184275\n",
            "0.6869165301322937 0.6873986721038818 0.4245495495495496 0.42567567567567566\n",
            "0.6865019798278809 0.6865718960762024 0.411496723996724 0.41216216216216217\n",
            "0.6864151358604431 0.685881495475769 0.4115479115479116 0.4077088452088452\n",
            "0.6860782504081726 0.68547123670578 0.40591728091728096 0.4021805896805897\n",
            "0.6856139898300171 0.6851328015327454 0.4044328419328419 0.3857493857493858\n",
            "0.6850690841674805 0.6848549246788025 0.39567977067977067 0.394041769041769\n",
            "0.6847702264785767 0.684607744216919 0.3983415233415234 0.3988022113022113\n",
            "0.6843966841697693 0.6837660670280457 0.3849303849303849 0.3855958230958231\n",
            "0.6839399337768555 0.6837242841720581 0.3785831285831286 0.37699631449631454\n",
            "0.683664083480835 0.6833789348602295 0.3810913185913186 0.36716830466830463\n",
            "0.6829771399497986 0.6820870637893677 0.3686527436527437 0.359029484029484\n",
            "0.682546854019165 0.6823917627334595 0.36716830466830463 0.36624692874692877\n",
            "0.6821293830871582 0.6816348433494568 0.36036036036036034 0.3545761670761671\n",
            "0.6822220087051392 0.6814719438552856 0.3653767403767404 0.35334766584766586\n",
            "0.6810980439186096 0.6804962754249573 0.3451064701064701 0.34398034398034394\n",
            "0.6811413168907166 0.6806206703186035 0.3518120393120393 0.34659090909090906\n",
            "0.6807377934455872 0.6810364723205566 0.34075552825552824 0.3527334152334153\n",
            "0.6803658604621887 0.6801593899726868 0.3383497133497133 0.3386056511056511\n",
            "0.6803640723228455 0.6793555617332458 0.334971334971335 0.32708845208845205\n",
            "0.6798551082611084 0.679088294506073 0.3356879606879607 0.3226351351351351\n",
            "0.6791769862174988 0.6784762740135193 0.3265253890253891 0.31772113022113024\n",
            "0.6789817214012146 0.6787917613983154 0.32360769860769856 0.32109950859950864\n",
            "0.6783007383346558 0.6775972247123718 0.31736281736281735 0.312039312039312\n",
            "0.6777828335762024 0.6776968836784363 0.31040131040131036 0.316492628992629\n",
            "0.6776847243309021 0.6773571372032166 0.31029893529893526 0.3121928746928747\n",
            "0.6769710183143616 0.6764857769012451 0.3065110565110565 0.30405405405405406\n",
            "0.6768162250518799 0.6760265827178955 0.30400286650286645 0.2983722358722358\n",
            "0.6765608191490173 0.6761980056762695 0.3032862407862408 0.29806511056511054\n",
            "0.6761555075645447 0.6756516695022583 0.296529484029484 0.2891584766584766\n",
            "0.6757521629333496 0.6751375794410706 0.30072686322686326 0.28731572481572487\n",
            "0.6755664348602295 0.6750130653381348 0.29463554463554464 0.29745085995085996\n",
            "0.6751871705055237 0.67472904920578 0.29105241605241605 0.2820945945945946\n",
            "0.6748065948486328 0.6741600036621094 0.28808353808353804 0.2863943488943489\n",
            "0.67436683177948 0.6737274527549744 0.2889025389025389 0.2802518427518428\n",
            "0.6739729642868042 0.6731453537940979 0.2870597870597871 0.2727272727272727\n",
            "0.6735439896583557 0.6735621690750122 0.2800470925470926 0.28117321867321865\n",
            "0.6730955839157104 0.6722261905670166 0.2772317772317773 0.273955773955774\n",
            "0.6727615594863892 0.6719948053359985 0.2765663390663391 0.27088452088452086\n",
            "0.6726389527320862 0.6720540523529053 0.27451883701883706 0.2728808353808354\n",
            "0.6721264123916626 0.6717613935470581 0.2696560196560197 0.269502457002457\n",
            "0.6717588901519775 0.671032726764679 0.27426289926289926 0.265970515970516\n",
            "0.671633780002594 0.6711931228637695 0.27180589680589684 0.2698095823095823\n",
            "0.671118438243866 0.670600414276123 0.26837633087633084 0.269041769041769\n",
            "0.6706182360649109 0.6704584360122681 0.2658169533169533 0.26182432432432434\n",
            "0.6702253222465515 0.6700344681739807 0.26540745290745293 0.2599815724815725\n",
            "0.6698625683784485 0.66912442445755 0.2637694512694513 0.26197788697788693\n",
            "0.6695837378501892 0.6691135764122009 0.26504914004914004 0.25844594594594594\n",
            "0.669134795665741 0.6686829328536987 0.2632063882063882 0.25552825552825553\n",
            "0.6685048341751099 0.6684680581092834 0.2602375102375102 0.25844594594594594\n",
            "0.66827791929245 0.6678414940834045 0.2561425061425061 0.2541461916461917\n",
            "0.668219804763794 0.6676740050315857 0.2605958230958231 0.2572174447174447\n",
            "0.6677031517028809 0.6676642298698425 0.25563063063063063 0.25506756756756754\n",
            "0.6675072312355042 0.6672726273536682 0.255579443079443 0.2532248157248157\n",
            "0.6669813394546509 0.6664990186691284 0.2539414414414415 0.2498464373464373\n",
            "0.6665524244308472 0.6656990051269531 0.25476044226044225 0.24815724815724816\n",
            "0.6662976145744324 0.6658695340156555 0.2533783783783784 0.25168918918918914\n",
            "0.6661396622657776 0.6658010482788086 0.25209868959868964 0.24815724815724816\n",
            "0.6656526923179626 0.6649411916732788 0.25194512694512694 0.24907862407862413\n",
            "0.6657590270042419 0.6654235124588013 0.2523546273546273 0.2503071253071253\n",
            "0.6649208068847656 0.6646251678466797 0.25194512694512694 0.24785012285012287\n",
            "0.6648448705673218 0.6641049981117249 0.2496416871416871 0.2467751842751843\n",
            "0.6645441651344299 0.663328230381012 0.25081900081900077 0.2469287469287469\n",
            "0.6639076471328735 0.663790762424469 0.249539312039312 0.2470823095823096\n",
            "0.6634820699691772 0.6631200313568115 0.24866912366912364 0.24493243243243246\n",
            "0.6635861396789551 0.6631236672401428 0.24907862407862413 0.24493243243243246\n",
            "0.6629289984703064 0.6626245975494385 0.2462633087633087 0.24554668304668303\n",
            "0.6627647876739502 0.6623960137367249 0.24805487305487306 0.2435503685503686\n",
            "0.662424623966217 0.6622219681739807 0.24779893529893526 0.24570024570024573\n",
            "0.6617639064788818 0.6612957715988159 0.24738943488943488 0.246468058968059\n",
            "0.6620314717292786 0.6604652404785156 0.2469287469287469 0.24201474201474205\n",
            "0.6615408658981323 0.6613196730613708 0.24713349713349708 0.24278255528255532\n",
            "0.6611841917037964 0.6604384183883667 0.24590499590499593 0.24278255528255532\n",
            "0.660785973072052 0.660064160823822 0.24539312039312045 0.24401105651105648\n",
            "0.6601762175559998 0.6597520709037781 0.24544430794430794 0.2430896805896806\n"
          ]
        }
      ]
    }
  ]
}